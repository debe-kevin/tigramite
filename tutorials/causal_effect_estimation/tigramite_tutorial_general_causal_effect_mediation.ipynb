{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19eece82",
   "metadata": {},
   "source": [
    "# Tutorial: Causal Mediation Analysis with `TIGRAMITE`\n",
    "\n",
    "TIGRAMITE is a time series analysis python module. It allows to reconstruct graphical models (conditional independence graphs) from discrete or continuously-valued time series based on the PCMCI framework and create high-quality plots of the results. \n",
    "\n",
    "This tutorial contains a number of examples introducing the use of the implemented mediation functionality and a \"technical appendix\" for more detailed setups.\n",
    "\n",
    "__Note:__ This implementation explicitely addresses not only time series, but also non-time series data.\n",
    "\n",
    "## Introduction and Contents\n",
    "\n",
    "Mediation analysis aims to understand the relative importance of different pathways and interactions in a graph.\n",
    "This tutorial serves both to illustrate the potential use-cases of causal mediation-analysis in general\n",
    "and through the implementation provided within the tigramite-package in particular.\n",
    "The tutorial is mostly based on demonstrating the use of the implementation and the analysis of qualitatively\n",
    "different effects in a number of simple yet informative examples.\n",
    "\n",
    "Counterfactual mediation results \\[1,2,3\\] can be very powerfull, but often much care should be taken in their interpretation. If possible, we recommend evaluating on toy-models to what degree certain phenomena are captured qualitatively or quantitatively (see Example 5).\n",
    "\n",
    "The implementation given here relies on adjustment-sets (combining \\[3\\] with, \\[4,5\\] for automatically generating candidate-sets). There are other methods of estimating mediation-effects (see e.g. \\[6,7\\] as a starting point) and were possible, validating different methods against each other seems important\n",
    "and recommendable.\n",
    "Counterfactual effect-estimation might require \"out-of-context\" falsification (e.g. through expert-knowledge), see for example \\[8\\].\n",
    "\n",
    "The default (function-level) fits used are mainly used for performance and interpretability reasons. Oftentimes there will be more suitable regressors for given data. To account for this, (function-level) fits can easily be replaced by other regressors (see appendix B).\n",
    "\n",
    "### The examples are:\n",
    "* [Example 0](#ex0): Direct starting point from the \"Causal Effects\" tutorial.\n",
    "\n",
    "Qualitative phenomena that are detectable (on a three-node \"triangle\" graph):\n",
    "* [Example 1](#ex1): Effect-Moderation.\n",
    "* [Example 2](#ex2): Effect on categorical Variables as shift in distribution.\n",
    "* [Example 3](#ex3): Saturation of Pathways passing through Categorical Variables.\n",
    "\n",
    "Graph-effects and time-series data:\n",
    "* [Example 4](#ex4): Simple \"feedback-loop\" system.\n",
    "* [Example 5](#ex5): A \"full-stack\" (causal discovery to interpreting mediation results) example.\n",
    "* [Example 6](#ex6): Unidentified effects.\n",
    "\n",
    "### The 'technical appendix' contains:\n",
    "* [A](#exA): Evaluate estimator performance on ensembles of toy-models.\n",
    "* [B](#exB): Extension-points for using different (function-)fit implementations.\n",
    "* [C](#exC): Inspecting fits and some pitfalls.\n",
    "\n",
    "### References:  \n",
    "\\[1\\] J. M. Robins and S. Greenland. Identifiability and exchangeability for direct and indirect effects. Epidemiology, 3(2):143–155, 1992.  \n",
    "\\[2\\] J. Pearl. Direct and indirect effects. Proceedings of the Seventeenth Conference on Uncertainty in Artificial intelligence, 2001.  \n",
    "\\[3\\] I. Shpitser and T. J. VanderWeele. A complete graphical criterion for the adjustment formula in mediation analysis. The international journal of biostatistics, 7(1), 2011.   \n",
    "\\[4\\] E. Perkovic, J. Textor, M. Kalisch, and M. H. Maathuis. Complete graphical characterization and construction of adjustment sets in markov equivalence classes of ancestral graphs. 2018.  \n",
    "\\[5\\] J. Runge. Necessary and sufficient graphical conditions for optimal adjustment sets in causal graphical models with hidden variables. Advances in Neural Information Processing Systems, 34:15762–15773, 2021.  \n",
    "\\[6\\] J. Pearl. Interpretation and identification of causal mediation. Psychological methods, 19(4):459, 2014.  \n",
    "\\[7\\] I. Shpitser. Counterfactual graphical models for longitudinal mediation analysis with unobserved confounding. Cognitive science, 37(6):1011–1035, 2013.  \n",
    "\\[8\\] J. M. Robins and T. S. Richardson. Alternative graphical causal models and the identification of direct effects. Causality and psychopathology: Finding the determinants of disorders and their cures, 84:103–58, 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ec0f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from tigramite import plotting as tp\n",
    "from tigramite.causal_effects import CausalEffects\n",
    "\n",
    "# Data and groundtruth generation from non-additive toymodels:\n",
    "import tigramite.toymodels.non_additive as toy_setup\n",
    "from tigramite.toymodels.non_additive import ContinuousVariable, CategoricalVariable, Environment, Model, World, PlotInfo\n",
    "\n",
    "# Standalone or direct use with tigramite via toy_setup.VariablesFromDataframe or toy_setup.DataframeFromVariables:\n",
    "import tigramite.causal_mediation as mediation\n",
    "\n",
    "# Extending tigramite's CausalEffects (see Example 0):\n",
    "from tigramite.causal_mediation import CausalMediation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd592bfd",
   "metadata": {},
   "source": [
    "# Example 0: Direct use with Tigramite <a id='ex0'></a>\n",
    "The mediation analysis can be used directly with tigramite data-types or with shorthands provided by the\n",
    "non-additive toymodel-implementation, see also Example 5 for easy-to-use data conversions.\n",
    "\n",
    "The following example is taken from the \"Effect Estimation\" tutorial, see section 'Causal effects in time series DAGs' there for details.\n",
    "The only modification required is to replace 'CausalEffects' by 'CausalMediation' (which extends the former).\n",
    "\n",
    "Note, that when using many continuous variables, depending on the fit-mechanism used (here nearest neighbor) it might be important to normalize data to get reasonable results. This applies (of course) in particular, if scales are different. For example setting the direct_effect to a huge value (e.g. 500) will completely invalidate the multi-dimensional nearest-neighbor fit: 'Nearest' in the isotropic n-dimensional space is then essentially the same as nearest in the (lagged) values of $X^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f1f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph =  np.array([[['', '-->', ''],\n",
    "                    ['', '', ''],\n",
    "                    ['', '', '']],\n",
    "                   [['', '-->', ''],\n",
    "                    ['', '-->', ''],\n",
    "                    ['-->', '', '-->']],\n",
    "                   [['', '', ''],\n",
    "                    ['<--', '', ''],\n",
    "                    ['', '-->', '']]], dtype='<U3')\n",
    "\n",
    "X = [(1,-2)]\n",
    "Y = [(2,0)]\n",
    "#causal_effects = CausalEffects(graph, graph_type='stationary_dag', X=X, Y=Y,\n",
    "#                               S=None, \n",
    "#                               hidden_variables=None, \n",
    "#                               verbosity=1)\n",
    "causal_effects = CausalMediation(graph, graph_type='stationary_dag', X=X, Y=Y,\n",
    "                                S=None, # (currently S must be None)\n",
    "                                hidden_variables=None, # (currently hidden must be None)\n",
    "                                verbosity=1)\n",
    "var_names = ['$X^0$', '$X^1$', '$X^2$']\n",
    "\n",
    "opt = causal_effects.get_optimal_set()\n",
    "print(\"Oset = \", [(var_names[v[0]], v[1]) for v in opt])\n",
    "special_nodes = {}\n",
    "for node in causal_effects.X:\n",
    "    special_nodes[node] = 'red'\n",
    "for node in causal_effects.Y:\n",
    "    special_nodes[node] = 'blue'\n",
    "for node in opt:\n",
    "    special_nodes[node] = 'orange'\n",
    "for node in causal_effects.M:\n",
    "    special_nodes[node] = 'lightblue'\n",
    "\n",
    "    \n",
    "tp.plot_time_series_graph(graph = causal_effects.graph,\n",
    "        var_names=var_names, \n",
    "#         save_name='Example.pdf',\n",
    "        figsize = (8, 4),\n",
    "        special_nodes=special_nodes\n",
    "        ); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2cd7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tigramite import data_processing as pp\n",
    "from tigramite.toymodels import structural_causal_processes as toys\n",
    "\n",
    "coeff = .5\n",
    "direct_eff = 0.5\n",
    "def lin_f(x): return x\n",
    "links_coeffs = {\n",
    "                0: [((0, -1), coeff, lin_f), ((1, -1), coeff, lin_f)], \n",
    "                1: [((1, -1), coeff, lin_f),], \n",
    "                2: [((2, -1), coeff, lin_f), ((1, 0), coeff, lin_f), ((1,-2), direct_eff, lin_f)],\n",
    "                }\n",
    "# Observational data\n",
    "T = 1000\n",
    "data, nonstat = toys.structural_causal_process(links_coeffs, T=T, noises=None, seed=None)\n",
    "normalization = []\n",
    "data_normalized = np.empty_like(data)\n",
    "for v in range(0,3):\n",
    "    m = np.std(data[:,v])\n",
    "    normalization.append(m)\n",
    "    data_normalized[:,v] = data[:,v] / m\n",
    "dataframe = pp.DataFrame(data, var_names=var_names)\n",
    "dataframe_normalized = pp.DataFrame(data_normalized, var_names=var_names)\n",
    "\n",
    "tp.plot_timeseries(dataframe); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc5498",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_setup = mediation.FitSetup(mediation.FitProvider_Continuous_Default.UseSklearn(20))\n",
    "\n",
    "# unnormalized data\n",
    "causal_effects.fit_natural_direct_effect(dataframe, blocked_mediators='all',\n",
    "                                mixed_data_estimator=fit_setup).PrintInfo()\n",
    "\n",
    "nde_est = causal_effects.predict_natural_direct_effect(0.0, 1.0)\n",
    "\n",
    "# normalized data\n",
    "causal_effects.fit_natural_direct_effect(dataframe_normalized, blocked_mediators='all',\n",
    "                                mixed_data_estimator=fit_setup)\n",
    "\n",
    "nde_est_from_normalized = causal_effects.predict_natural_direct_effect(0.0, 1.0) * normalization[2] / normalization[1]\n",
    "\n",
    "# print results\n",
    "print( f\"Estimate of the NDE is:\\n{nde_est} from unnormalized data, \"\n",
    "     + f\"\\n{nde_est_from_normalized} from normalized data,\\nground-truth is {direct_eff}.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477eba1a",
   "metadata": {},
   "source": [
    "While normalization helps (especially with large scale-differences), for large adjustment and mediator sets  high-dimensional fits are required, and nearest neighbor regressors seem to be of only limited use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142f7af7",
   "metadata": {},
   "source": [
    "# Example 1: Effect-Moderation <a id='ex1'></a>\n",
    "The effect of X on Y is moderated by the synergistic/interaction term proportional to the *product* mx. This is a particularly simple example of a non-additive, synergistic model, but it already illustrates the effect of \"moderation\", which makes causal effects non-constant (they change with the reference-value of X).\n",
    "\n",
    "In the example below, at X=0, M should be normal-distributed with mean 0, so the equation for Y effectively (up to noise) reads Y = X.\n",
    "Hence the NDE for changing X from 0 to 1, keeping M as if X were still 0, should be simply 1.0.\n",
    "\n",
    "At X=1, M should be normal-distributed with mean 0.7 (from adding 0.7 X = 0.7), so the equation for Y\n",
    "effectively reads Y = (1.0 + 0.7 * 0.7) X = 1.49 * X.\n",
    "Hence the NDE for changing X from 1 to 2, keeping M as if X were still 1, should now be 1.49.\n",
    "\n",
    "At X=-2, M should be normal-distributed with mean -1.4 (from adding 0.7 X = -1.4), so the equation for Y\n",
    "effectively reads Y = (1.0 - 0.98) X = 0.02 * X.\n",
    "Hence the NDE for changing X from -2 to 1, should almost vanish.\n",
    "\n",
    "While this is probably the conceptually simplest example, the actual estimator-performance is relatively noisy.\n",
    "It correctly indicates the qualitative change (the NDE increases with increasing reference-value for X), but quantiative results are rather uncertain, especially for larger values of x. Estimating the effect at many intermediate points along the x-axis (here 100) and smoothing the result gives a better result. Note that further away from $0$, there are less training-samples and the estimate becomes worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a91ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ContinuousVariable()\n",
    "Y = ContinuousVariable()\n",
    "M = ContinuousVariable()\n",
    "\n",
    "env = Environment(exogenous_noise = {\n",
    "    X : lambda rng, N : 2.0 * rng.standard_normal(N),    \n",
    "    M : lambda rng, N : rng.standard_normal(N),\n",
    "    Y : lambda rng, N : rng.standard_normal(N)\n",
    "}, N=1000, seed=123)\n",
    "model = Model(sem = {\n",
    "    X : lambda value : value[X.Noise()],\n",
    "    M : lambda value : 0.7 * value[X] + value[M.Noise()],\n",
    "    Y : lambda value : (1.0 + 0.7 * value[M]) * value[X] + value[Y.Noise()]\n",
    "})\n",
    "\n",
    "fit_setup = mediation.FitSetup()\n",
    "#ensemble = toy_setup.Ensemble(shared_setup=env, payloads=[ComputeNDE, ComputeGroundTruth01], runs=1000)\n",
    "\n",
    "world = World(env, model)\n",
    "estimator = mediation.NaturalEffects_StandardMediationSetup(fit_setup, source=X, target=Y, mediator=M,\n",
    "                                                data=world.Observables())\n",
    "\n",
    "grid = np.arange(-2.0, 2.8, 0.8)\n",
    "nde_fct_est = estimator.NDE_grid(grid, normalize_by_delta=True)\n",
    "x_values, nde_fct_est_smoothed = estimator.NDE_smoothed(-2.0, 2.0, normalize_by_delta=True, cf_delta=0.8)\n",
    "nde_fct_ground_truth = toy_setup.GroundTruth_NDE_fct(X, Y, M, env, model, grid, normalize_by_delta=True, cf_delta=0.8)\n",
    "\n",
    "plt.plot(np.arange(-2.0, 2.8, 0.8), nde_fct_est, color=\"skyblue\", label=\"estimate\")\n",
    "plt.plot(x_values, nde_fct_est_smoothed, color=\"dodgerblue\", label=\"estimate (smoothed)\")\n",
    "plt.plot(np.arange(-2.0, 2.8, 0.8), nde_fct_ground_truth, color=\"tab:orange\", label=\"ground truth\")\n",
    "plt.legend()\n",
    "plt.title(f\"Effect Moderation\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"NDE\")\n",
    "#plt.savefig(\"Example1.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149fdb4",
   "metadata": {},
   "source": [
    "# Example 2: Effect on categorical Variables as shift in distribution<a id='ex2'></a>\n",
    "\n",
    "Intervening on the value of X really is not just a change in expectation value, but in distribution.\n",
    "Thus so are effects.\n",
    "We illustrate this for the effect on a catgorical outcome Y with 3 categories.\n",
    "\n",
    "The distribution over the three categories in the output is fully described by three numbers:\n",
    "The probabilities for the occurence of each respective category:\n",
    "[$p_0$, $p_1$, $p_2$].\n",
    "The NDE is thus expressed as the expected change in these probabilites, when changing the value of X\n",
    "while keeping M as if X had taken the original values.\n",
    "These *shifts* in the individual probabilities can of course be negative, actually, since\n",
    "$1 = p_0 + p_1 + p_2$, both before and after chaning X, the shifts should add up to 0.\n",
    "\n",
    "In the example below \"higher\" categories are more likely for higher X. For small values of X, Y is likely to be in category 0 or 1, thus the effect of changing X,\n",
    "while X is small, mostly consists of shifting occurences of category 0 to occurences of 1.\n",
    "Thus the $p_0$ \"loses\" while $p_1$ \"gains\" in value.\n",
    "\n",
    "Then for larger x occurences of category 1 shift to category 2. Additionally at some point $p_0$ is already close to $0$ and there is not much left to shift to category 1 from here. Hence category 1 \"passes along\" events to category 2 while not \"receiving\" new ones from category 0 anymore. As a result, now $p_1$ starts to drop,\n",
    "while $p_2$ increases.\n",
    "\n",
    "At some point (for very small or very large x), there is not much changing anymore: Basically all events occur in category 0 or 2 respectively. See also the next example 3.\n",
    "\n",
    "All of this behavior is qualitatively very well represented by the results.\n",
    "\n",
    "Note: The ground truth here looks more \"noisy\" than the estimate. This is because the estimate is smoothed over many estimates along the x-axis, while the ground-truth is \"honest\" with respect to noise in the system: It really computes the counterfactual world (for the 1000 sample points of noise given), which *is* noisy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f15c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ContinuousVariable()\n",
    "Y = CategoricalVariable(categories=3)\n",
    "M = ContinuousVariable()\n",
    "\n",
    "env = Environment(exogenous_noise = {\n",
    "    X : lambda rng, N : 2.0 * rng.standard_normal(N),    \n",
    "    M : lambda rng, N : rng.standard_normal(N),\n",
    "    Y : lambda rng, N : rng.standard_normal(N)\n",
    "}, N=1000, seed=12345)\n",
    "model = Model(sem = {\n",
    "    X : lambda value : value[X.Noise()],\n",
    "    M : lambda value : 0.5 * value[X] + value[M.Noise()],\n",
    "    Y : lambda value : np.round( np.clip(\n",
    "            0.2 * value[Y.Noise()] + value[X] + value[M],\n",
    "        a_min=-0.49, a_max=2.49 ) )\n",
    "})\n",
    "\n",
    "world = World(env, model)\n",
    "\n",
    "fit_setup = mediation.FitSetup()\n",
    "estimator = mediation.NaturalEffects_StandardMediationSetup(fit_setup, source=X, target=Y, mediator=M,\n",
    "                                                    data=world.Observables())\n",
    "\n",
    "\n",
    "x_smoothed, nde_fct_est_smoothed = estimator.NDE_smoothed(-2.0, 2.0, normalize_by_delta=True, steps=100)\n",
    "grid = np.arange(-3.0, 3.06, 0.06)\n",
    "nde_fct_ground_truth = toy_setup.GroundTruth_NDE_fct(X, Y, M, env, model, grid, normalize_by_delta=True)\n",
    "\n",
    "plt_info_est = PlotInfo(color=\"tab:blue\", label=\"NDE (Est)\", colorTE=\"dodgerblue\", colorCF=\"skyblue\",\n",
    "                        labelTE=\"TE (Est)\", labelCF=\"CF (Est)\")\n",
    "plt_info_gt = PlotInfo(color=\"tab:orange\", label=\"NDE (GT)\", colorTE=\"orange\", colorCF=\"navajowhite\",\n",
    "                       labelTE=\"TE (GT)\", labelCF=\"CF (GT)\")\n",
    "\n",
    "plt_data_est = PlotInfo(x=x_smoothed, y=nde_fct_est_smoothed, **plt_info_est)\n",
    "plt_data_gt = PlotInfo(x=grid, y=nde_fct_ground_truth, **plt_info_gt)\n",
    "\n",
    "fig = toy_setup.PlotAbsProbabilities(plt, Y, [plt_data_est, plt_data_gt],\n",
    "                labels=PlotInfo(x=\"x\", y=\"P(Y={cY})\",\n",
    "                title=r\"Values of $P(Y_{x',M_x})$ (light) and total effect $= P(Y_{x,M_x})$ (dark) as functions of $x$\"))\n",
    "\n",
    "fig = toy_setup.PlotChangeInProbabilities(plt, Y, [plt_data_est, plt_data_gt],\n",
    "                labels=PlotInfo(x=\"x\", y=\"ΔP(Y={cY})\",\n",
    "                title=r\"Change in $P(Y_{x', M_x})$ from changing $x'$, as function of $x$\"))\n",
    "\n",
    "\n",
    "#plt.savefig(\"Example2.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9168c5",
   "metadata": {},
   "source": [
    "# Example 3: Saturation of Pathways passing through Categorical Variables <a id='ex3'></a>\n",
    "\n",
    "Here we look at the *indirect* effect, i.e. the effect \"passing through\" the mediator M.\n",
    "This illustrates, how categorical variables behave non-linearly even on average:\n",
    "One might think, that an increase of X can change the odds of categories in M linearly, so that *expectations* for the outcome change approximately linear as well. This can indeed happen, as long as the probabilites of the differenent categories of M are sufficiently balanced. However, at some point (e.g. for large enough X), M will \"always\" be in the same category anyway, so that further changing X cannot change that pathway anymore.\n",
    "This leads to saturation of pathways through categorical mediators.\n",
    "\n",
    "The noise in this example is chosen to be $\\gamma$-distributed, mostly because otherwise the ground-truth would be a Gauß-curve, and using Gauß-smoothing on estimates seemed somewhat dubious for that case.\n",
    "This means noise-values are non-negative.\n",
    "\n",
    "Here, $M$ is simply given as follows: If $X + \\eta_M > 1$ (where $\\eta_M$ is the non-negative noise on $M$),\n",
    "then $M=1$, otherwise $M=0$. Thus higher values of $X$ increase the odds of $M=1$ as compared to $M=0$.\n",
    "\n",
    "For small values of $X$, $M$ is still well below the threshold (relative to the 'typical' noise on $M$),\n",
    "hence $X + \\eta_M$ is typically \"small\" compared to 1, and $P(X + \\eta_M > 1)$ is small.\n",
    "\n",
    "Approaching $X=1$, due to non-negativity of the noise on M, $P(X + \\eta_M > 1)$ approaches one.\n",
    "\n",
    "Beyond, $X>1$, always $P(X + \\eta_M > 1)$, so moving $M$ further away from the threshold does not change the already saturated M any more.\n",
    "\n",
    "Note: Smoothing the estimate more captures the qualitative form better, but also smooths out sharp features of the actual (ground-truth) result: For example the point in $X$ where the indirect effect approaches zero is gets biased away from the ground-truth value ($X=1$, see above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad27f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ContinuousVariable()\n",
    "Y = ContinuousVariable()\n",
    "M = CategoricalVariable(categories=2)\n",
    "\n",
    "env = Environment(exogenous_noise = {\n",
    "    X : lambda rng, N : 2.0 * rng.standard_gamma(2.0,size=N)-3.0,    \n",
    "    M : lambda rng, N : rng.standard_gamma(2.0,size=N),\n",
    "    Y : lambda rng, N : rng.standard_normal(N)\n",
    "}, N=10000, seed=42 )\n",
    "model = Model(sem = {\n",
    "    X : lambda value : value[X.Noise()],\n",
    "    M : lambda value : value[X] + value[M.Noise()] > 1.0, # M is binary, ie true or false\n",
    "    Y : lambda value : value[X] + value[M] + value[Y.Noise()]\n",
    "})\n",
    "                  \n",
    "world = World(env, model)\n",
    "\n",
    "fit_setup = mediation.FitSetup()\n",
    "\n",
    "estimator = mediation.NaturalEffects_StandardMediationSetup(fit_setup, source=X, target=Y, mediator=M,\n",
    "                                                    data=world.Observables())\n",
    "\n",
    "grid = np.arange(-3.0, 3.10, 0.05)\n",
    "x_values2, nie_est_smoothed2 = estimator.NIE_smoothed(-3.0, 3.0, steps=100, smoothing_gaussian_sigma_in_steps=2,\n",
    "                                                      normalize_by_delta=True)\n",
    "x_values5, nie_est_smoothed5 = estimator.NIE_smoothed(-3.0, 3.0, steps=100, smoothing_gaussian_sigma_in_steps=5,\n",
    "                                                      normalize_by_delta=True)\n",
    "nie_ground_truth = toy_setup.GroundTruth_NIE_fct(X, Y, M, env, model, grid, normalize_by_delta=True)\n",
    "\n",
    "plt.plot(x_values2, nie_est_smoothed2, color=\"forestgreen\",\n",
    "         label=\"Estimate from data (slightly smoothed)\")\n",
    "plt.plot(x_values5, nie_est_smoothed5, color=\"darkgreen\",\n",
    "         label=\"Estimate from data (more smoothed)\")\n",
    "plt.plot(grid, nie_ground_truth, color=\"tab:orange\",\n",
    "         label=\"Ground Truth (unsmoothed)\")\n",
    "plt.legend()\n",
    "plt.xlabel(r\"$X$\")\n",
    "plt.ylabel(r\"NIE\")\n",
    "plt.title(\"Natural Indirect Effect\")\n",
    "#plt.savefig(\"Example3.pdf\", format=\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be28f159",
   "metadata": {},
   "source": [
    "# Example 4: (Time-Series) Graphs - Feedback Loops <a id='ex4'></a>\n",
    "Effect mediation in more complex graphs, e.g. in time-series graphs, is especially interessting. It requires adjustment-sets, which *if they exist*, can automatically be found and used by the implementation.\n",
    "However, this makes fits more challenging and may not be possible for arbitrary effects in generic graphs.\n",
    "\n",
    "The example below generates a time-series for a feed-back loop system. For example think of the (continuous) X as the temperature, and the (binary) M indicating the presence of a snow-cover. These (over time) are known to affect each other in a reinforcing way.\n",
    "The natrual direct effect estimated here is the effect of temperature on future temperature (i.e. its \"inertness\") as opposed to the feedback through (albedo change) due to snow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de793f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model\n",
    "\n",
    "X = toy_setup.ContinuousVariable(\"X\")\n",
    "M = toy_setup.CategoricalVariable(\"M\", categories=2)\n",
    "coeff = 0.8\n",
    "\n",
    "env = toy_setup.Environment(exogenous_noise={\n",
    "    X: lambda rng, N: 1.0 * rng.standard_normal(N),\n",
    "    M: lambda rng, N: 1.0 * rng.standard_normal(N),\n",
    "}, N=5000, seed=11)\n",
    "model = toy_setup.Model(sem={\n",
    "    X: lambda value: value[X.Noise()] + 0.5 * value[M.Lag(1)] + coeff * value[X.Lag(1)],\n",
    "    M: lambda value: value[X] + value[M.Noise()] + 0.7 * value[M.Lag(1)] > 1.0  # M is binary, ie true or false\n",
    "})\n",
    "\n",
    "# The Groundtruth Graph\n",
    "graph, graph_type = model.GetGroundtruthGraph()\n",
    "\n",
    "print(f\"The Ground-Truth Graph (type is '{graph_type}')\")\n",
    "print(\"As a summary-graph:\")\n",
    "tp.plot_graph(graph = graph,\n",
    "        figsize = (5, 2),\n",
    "        var_names = [\"X\", \"M\"]\n",
    "        ); plt.show()\n",
    "\n",
    "print(\"As a time series-graph:\")\n",
    "tp.plot_time_series_graph(graph = graph,\n",
    "        figsize = (5, 2),\n",
    "        ); plt.show()\n",
    "\n",
    "# Generate data\n",
    "world = toy_setup.World(env, model)\n",
    "obs = world.Observables()\n",
    "\n",
    "# Fit & Estimator Setup\n",
    "fit_setup = mediation.FitSetup(mediation.FitProvider_Continuous_Default.UseSklearn(20))\n",
    "tau_max = 2\n",
    "\n",
    "print(\"\\nAutomatically setting up an estimator\")\n",
    "estimator = mediation.NaturalEffects_GraphMediation(graph, graph_type, tau_max, fit_setup, world.Observables(),\n",
    "                                                  effect_source=X.Lag(1), effect_target=X,\n",
    "                                                  blocked_mediators=\"all\", adjustment_set=\"auto\")\n",
    "estimator.PrintInfo()\n",
    "\n",
    "print(\"\\nManually setting up an estimator\")\n",
    "estimator2 = mediation.NaturalEffects_GraphMediation(graph, graph_type, tau_max, fit_setup, world.Observables(),\n",
    "                                                   effect_source=X.Lag(1), effect_target=X,\n",
    "                                                   blocked_mediators=[M.Lag(1)], adjustment_set=[M.Lag(2)])\n",
    "estimator2.PrintInfo()\n",
    "print(\"\")\n",
    "\n",
    "\n",
    "# Compute Single-Value Estimates\n",
    "nde01 = estimator.NDE(0.0, 1.0)\n",
    "nde01_ = estimator2.NDE(0.0, 1.0)\n",
    "nde01_gt = toy_setup.GroundTruth_NDE(0.0, 1.0, X.Lag(1), X, [M.Lag(1)], env, model)\n",
    "print( f\"Estimated NDE at x=0 is {nde01:.3f} (auto) = {nde01_:.3f} (manual), analytical NDE is {coeff:.3f}.\")\n",
    "print( f\"[Numerical Ground-Truth is {nde01_gt:.3f}]\")\n",
    "\n",
    "\n",
    "# Compute Function-Estimates\n",
    "x_min = -2.0\n",
    "x_max = 6.0\n",
    "x_values, nde_est_smoothed = estimator.NDE_smoothed(x_min, x_max, normalize_by_delta=True, cf_delta=.5,\n",
    "                                                   smoothing_gaussian_sigma_in_steps=5)\n",
    "\n",
    "\n",
    "x_values_gt, nde_fct_gt = toy_setup.GroundTruth_NDE_fct_auto(x_min, x_max, estimator, env, model,\n",
    "                                            cf_delta=.5, normalize_by_delta=True)\n",
    "\n",
    "\n",
    "# Plot Function-Estimates\n",
    "\n",
    "plt.plot(x_values, nde_est_smoothed, color=\"tab:blue\",\n",
    "         label=\"Estimate from data (smoothed)\")\n",
    "plt.plot(x_values_gt, nde_fct_gt, color=\"tab:orange\",\n",
    "         label=\"Ground-Truth (numerical)\")\n",
    "\n",
    "plt.title(\"Natural Direct Effect in (ts-)Graph\")\n",
    "plt.xlabel(r\"Present value of $A$\")\n",
    "plt.ylabel(r\"NDE on Future value of $A$\")\n",
    "\n",
    "# (also show where we actually have data)\n",
    "x = obs[X]\n",
    "counts, bins = np.histogram(x[np.logical_and(x > x_min, x < x_max)], 50)\n",
    "max_counts = np.max(counts)\n",
    "plt.stairs(counts/max_counts, bins, color='tab:gray', label=\"Data Availabililty (est. from x only)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a27e2",
   "metadata": {},
   "source": [
    "# Example 5: Exploring a more Complicated Model <a id='ex5'></a>\n",
    "We set up a simple model (for illustrating the use of mediation methods, this is not supposed to be scientifically meaningful), that describes how \"(non-)cloudedness\" $X$ causes both higher temperature $M_1$ and higher likelihood of droughts $M_2$, which in turn both affect (via a reinforcing interaction) the wild-fire-risc $Y$.\n",
    "\n",
    "Additionally temperature $M_1$ is assumed to also affect the occurences of droughts $M_2$. To make things more interessting, this effect is chosen, such that a drought is \"guaranteed\" to occur above a certain temperature-threshold.\n",
    "\n",
    "The different pathways are not balanced, i.e. some are more important than others (see below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee6116",
   "metadata": {},
   "source": [
    "## 1. Set up the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef3981e",
   "metadata": {},
   "source": [
    "Categorical variables are not only often intuitive, they also often perform better numerically (both in terms of computation speed and precision). Further, e.g. when (ML) preprocessing is used, having mixed-type (continuous and categorical variables) can be unavoidable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6d8be5",
   "metadata": {},
   "source": [
    "### 1.1 Variables and SEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9810b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables\n",
    "X = toy_setup.CategoricalVariable(\"Clouded\", categories=2)\n",
    "M1 = toy_setup.ContinuousVariable(\"Temperature\")\n",
    "M2 = toy_setup.CategoricalVariable(\"Drought\", categories=2)\n",
    "Y = toy_setup.CategoricalVariable(\"Wild-Fire\", categories=2)\n",
    "\n",
    "# The Model\n",
    "autocorr = 0.3\n",
    "temp_cutoff = .5\n",
    "model = toy_setup.Model(sem={\n",
    "    X: lambda value:\n",
    "        autocorr * value[X.Lag(1)] + value[X.Noise()] > 0.5,\n",
    "    M1: lambda value:\n",
    "        autocorr * value[M1.Lag(1)] + (2.0 * value[X.Lag(1)] - 1.0) + value[M1.Noise()],\n",
    "    M2: lambda value:\n",
    "        np.square(autocorr * value[M2.Lag(1)] + value[X.Lag(1)] + value[M2.Noise()])\n",
    "        > (temp_cutoff - value[M1.Lag(1)]),\n",
    "    Y: lambda value:\n",
    "        np.logical_or( autocorr * value[Y.Lag(1)] + value[M1.Lag(1)] + value[Y.Noise()] > 2.5,\n",
    "            np.logical_and( value[M2.Lag(1)],\n",
    "                autocorr * value[Y.Lag(1)] + 5.0 * (value[M1.Lag(1)]+0.5) + value[Y.Noise()] > 2.5 ) )\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d4ba24",
   "metadata": {},
   "source": [
    "### 1.2 Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e466b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unobserved, exogenous noise (environment)\n",
    "env = toy_setup.Environment(exogenous_noise={\n",
    "    X: lambda rng, N: 1.0 * rng.standard_normal(N),\n",
    "    M1: lambda rng, N: 1.0 * rng.standard_normal(N),\n",
    "    M2: lambda rng, N: 1.0 * rng.standard_normal(N),\n",
    "    Y: lambda rng, N: 1.0 * rng.standard_normal(N)\n",
    "}, N=500, seed=11)\n",
    "\n",
    "# The \"world\" to observe data from\n",
    "world = toy_setup.World(env, model)\n",
    "obs = world.Observables()\n",
    "\n",
    "# The \"dataframe\" for use with tigramite\n",
    "dataframe = toy_setup.DataframeFromVariables(obs)\n",
    "\n",
    "#Plot the data\n",
    "tp.plot_timeseries(dataframe); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfe1fbd",
   "metadata": {},
   "source": [
    "## 2. The Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b02665",
   "metadata": {},
   "source": [
    "### 2.1 Ground-Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb4ed48",
   "metadata": {},
   "source": [
    "Note, that the ground-truth generation does not validate \"faithfullness\", e.g. setting autocorr=0 will still produce auto-lags in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0ff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get from model\n",
    "graph, graph_type = model.GetGroundtruthGraph()\n",
    "\n",
    "print(f\"The ground truth graph (type={graph_type}):\")\n",
    "\n",
    "# Plot summary graph\n",
    "print(\"Summary Graph:\")\n",
    "tp.plot_graph(graph=graph,\n",
    "        figsize = (5, 2),\n",
    "        var_names = [\"X\", \"M1\", \"M2\", \"Y\"]\n",
    "        ); plt.show()\n",
    "\n",
    "# Plot Time series graph\n",
    "print(\"Time series graph:\")\n",
    "tp.plot_time_series_graph(graph=graph,\n",
    "        figsize = (5, 2),\n",
    "        ); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3c3926",
   "metadata": {},
   "source": [
    "### 2.2 From Data via Causal Discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e718cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tigramite.pcmci import PCMCI\n",
    "# from tigramite.independence_tests.cmiknnmixed import CMIknnMixed\n",
    "from tigramite.independence_tests.regressionCI import RegressionCI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2edda",
   "metadata": {},
   "source": [
    "#### (a) Using \"regression CI\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1d5d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "indep_test = RegressionCI()\n",
    "tau_max = 2\n",
    "pcmci = PCMCI(\n",
    "    dataframe=dataframe, \n",
    "    cond_ind_test=indep_test)\n",
    "results = pcmci.run_pcmci(tau_max=tau_max, pc_alpha=0.1, alpha_level=0.01)\n",
    "graph_cd = results['graph']\n",
    "\n",
    "\n",
    "print(\"Graph found with RegressionCI is:\")\n",
    "# Plot summary graph\n",
    "print(\"Summary Graph:\")\n",
    "tp.plot_graph(\n",
    "    val_matrix=results['val_matrix'],\n",
    "    graph=results['graph'],\n",
    "    figsize = (5, 2),\n",
    "    var_names=[\"X\", \"M1\", \"M2\", \"Y\"],\n",
    "    link_colorbar_label='cross-MCI',\n",
    "    node_colorbar_label='auto-MCI',\n",
    "    show_autodependency_lags=False\n",
    "    ); plt.show()\n",
    "\n",
    "# Plot Time series graph\n",
    "print(\"Time series graph:\")\n",
    "tp.plot_time_series_graph(\n",
    "    val_matrix=results['val_matrix'],\n",
    "    graph=results['graph'],\n",
    "    figsize = (5, 2),\n",
    "    var_names=dataframe.var_names #[\"X\", \"M1\", \"M2\", \"Y\"]\n",
    "    ); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839a49cb",
   "metadata": {},
   "source": [
    "#### (b) Using \"CMIknnMixed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979240de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use fewer data-points, because this test tends to be slow\n",
    "\n",
    "# # Unobserved, exogenous noise (environment)\n",
    "# env.ResetWithNewSeed(new_seed=11, N=5000)\n",
    "\n",
    "# # The \"world\" to observe data from\n",
    "# world = toy_setup.World(env, model)\n",
    "# obs = world.Observables()\n",
    "\n",
    "# # The \"dataframe\" for use with tigramite\n",
    "# dataframe = toy_setup.DataframeFromVariables(obs)\n",
    "\n",
    "# indep_test = CMIknnMixed(significance='shuffle_test', sig_samples=50) # (you should use more than 50 ...)\n",
    "# tau_max = 2\n",
    "# pcmci = PCMCI(\n",
    "#     dataframe=dataframe, \n",
    "#     cond_ind_test=indep_test)\n",
    "# results = pcmci.run_pcmci(tau_max=tau_max, pc_alpha=0.1, alpha_level=0.01)\n",
    "# graph_cd = results['graph']\n",
    "\n",
    "\n",
    "# print(f\"Graph found with CMIknnMixed is:\")\n",
    "# # Plot summary graph\n",
    "# print(\"Summary Graph:\")\n",
    "# tp.plot_graph(\n",
    "#     val_matrix=results['val_matrix'],\n",
    "#     graph=results['graph'],\n",
    "#     figsize = (5, 2),\n",
    "#     var_names=[\"X\", \"M1\", \"M2\", \"Y\"],\n",
    "#     link_colorbar_label='cross-MCI',\n",
    "#     node_colorbar_label='auto-MCI',\n",
    "#     show_autodependency_lags=False\n",
    "#     ); plt.show()\n",
    "\n",
    "# # Plot Time series graph\n",
    "# print(\"Time series graph:\")\n",
    "# tp.plot_time_series_graph(\n",
    "#     val_matrix=results['val_matrix'],\n",
    "#     graph=results['graph'],\n",
    "#     figsize = (5, 2),\n",
    "#     var_names=[\"X\", \"M1\", \"M2\", \"Y\"]\n",
    "#     ); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed6bf4f",
   "metadata": {},
   "source": [
    "Not quite the ground-truth graph, but the assigned importance of links agrees very well with the mediation-results (see 3.3e), in particular all \"important\" links are found and all links that are found with large confidence are actually in the model (and important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08fdc1a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# For copy-paste without rerunning\n",
    "print(results['graph'])\n",
    "print(results['val_matrix'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbeecfe",
   "metadata": {},
   "source": [
    "## 3. Mediation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d7ce9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the number of data-points\n",
    "\n",
    "# Unobserved, exogenous noise (environment)\n",
    "env.ResetWithNewSeed(new_seed=11, N=5000)\n",
    "\n",
    "# The \"world\" to observe data from\n",
    "world = toy_setup.World(env, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48bc26",
   "metadata": {},
   "source": [
    "### 3.1 Set up the fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bbf36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_setup = mediation.FitSetup(mediation.FitProvider_Continuous_Default.UseSklearn(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87f045",
   "metadata": {},
   "source": [
    "### 3.2 Set up (an) Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8990f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_max = 4\n",
    "estimator = mediation.NaturalEffects_GraphMediation(graph, graph_type, tau_max, fit_setup, world.Observables(),\n",
    "                                                  effect_source=X.Lag(2), effect_target=Y,\n",
    "                                                  blocked_mediators=\"all\", adjustment_set=\"auto\")\n",
    "    \n",
    "estimator.PrintInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd85430e",
   "metadata": {},
   "source": [
    "### 3.3 Analyize the Effect of X on Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a834282",
   "metadata": {},
   "source": [
    "#### (a) Both mediators blocked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009ac85e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute the NDE of flipping X from 0 to 1\n",
    "nde01 = estimator.NDE(False, True)\n",
    "\n",
    "# Generate Ground-Truth from environment with (perfect) knowledge of the model\n",
    "nde01_gt = toy_setup.GroundTruth_NDE(False, True, X.Lag(2), Y, [M1.Lag(1),M2.Lag(1)], env, model)\n",
    "# We could also just copy the settings from the estimator:\n",
    "nde01_gt_ = toy_setup.GroundTruth_NDE_auto(False, True, estimator, env, model)\n",
    "assert np.all(nde01_gt == nde01_gt_)\n",
    "\n",
    "\n",
    "print( f\"Before change P(fire)={nde01[1][1]:.3f} [ground-truth: {nde01_gt[1][1]:.3f}].\")\n",
    "print( f\"After change P(fire)={nde01[1][0]:.3f} [ground-truth: {nde01_gt[1][0]:.3f}].\" )\n",
    "print( f\"Thus fire-risc increased as a direct effect of X by {nde01[1][0] - nde01[1][1]:.3f} [gt: {nde01_gt[1][0] - nde01_gt[1][1]:.3f}].\\n\" )\n",
    "both_blocked = nde01[1][0] - nde01[1][1]\n",
    "both_blocked_gt = nde01_gt[1][0] - nde01_gt[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a64d55",
   "metadata": {},
   "source": [
    "#### (b) Blocking only $M_1$ (temperature), resulting in drought-mediated effect only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3844202c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an estimator for this effect\n",
    "estimator2 = mediation.NaturalEffects_GraphMediation(graph, graph_type, tau_max, fit_setup, world.Observables(),\n",
    "                                                  effect_source=X.Lag(2), effect_target=Y,\n",
    "                                                  blocked_mediators=[M1.Lag(1)], adjustment_set=\"auto\")\n",
    "\n",
    "nde01 = estimator2.NDE(0, 1) # Writing False/True or 0/1 should both work just fine \n",
    "nde01_gt = toy_setup.GroundTruth_NDE_auto(False, True, estimator2, env, model)\n",
    "print( f\"Before change P(fire)={nde01[1][1]:.3f} [ground-truth: {nde01_gt[1][1]:.3f}].\")\n",
    "print( f\"After change P(fire)={nde01[1][0]:.3f} [ground-truth: {nde01_gt[1][0]:.3f}].\" )\n",
    "print( f\"Thus fire-risc increased as a direct+drought-induced effect of non-cloudiness by {nde01[1][0] - nde01[1][1]:.3f} [gt: {nde01_gt[1][0] - nde01_gt[1][1]:.3f}].\\n\" )\n",
    "M1_blocked = nde01[1][0] - nde01[1][1]\n",
    "M1_blocked_gt = nde01_gt[1][0] - nde01_gt[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119b8d0",
   "metadata": {},
   "source": [
    "Remark: The adjustment-set does not depend on which mediators are blocked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b32c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator2.PrintInfo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56405f41",
   "metadata": {},
   "source": [
    "#### (c) Blocking only $M_2$ resulting in the temperature-mediated effect only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ba5eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator3 = mediation.NaturalEffects_GraphMediation(graph, graph_type, tau_max, fit_setup, world.Observables(),\n",
    "                                                effect_source=X.Lag(2), effect_target=Y,\n",
    "                                                blocked_mediators=[M2.Lag(1)], adjustment_set=\"auto\")\n",
    "\n",
    "nde01 = estimator3.NDE(0, 1) \n",
    "nde01_gt = toy_setup.GroundTruth_NDE_auto(False, True, estimator3, env, model)\n",
    "print( f\"Before change P(fire)={nde01[1][1]:.3f} [ground-truth: {nde01_gt[1][1]:.3f}]\" )\n",
    "print( f\"After change P(fire)={nde01[1][0]:.3f} [gt: {nde01_gt[1][0]:.3f}].\" )\n",
    "print( f\"Thus fire-risc increased as a direct+temperature-induced effect of non-cloudiness by {nde01[1][0] - nde01[1][1]:.3f} [gt: {nde01_gt[1][0] - nde01_gt[1][1]:.3f}].\\n\" )\n",
    "\n",
    "M2_blocked = nde01[1][0] - nde01[1][1]\n",
    "M2_blocked_gt = nde01_gt[1][0] - nde01_gt[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e296a",
   "metadata": {},
   "source": [
    "#### (d) Blocking no mediators resulting in the total effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6941b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up an estimator for this effect    \n",
    "estimator4 = mediation.NaturalEffects_GraphMediation(graph, graph_type, tau_max, fit_setup, world.Observables(),\n",
    "                                                  effect_source=X.Lag(2), effect_target=Y,\n",
    "                                                  blocked_mediators=[], adjustment_set=\"auto\",\n",
    "# Flag for fallback (avoid warning):\n",
    "                                                  fall_back_to_total_effect=True)\n",
    "\n",
    "nde01 = estimator4.NDE(False, True)\n",
    "nde01_gt = toy_setup.GroundTruth_NDE_auto(False, True, estimator4, env, model)\n",
    "\n",
    "print( f\"Before change P(fire)={nde01[1][1]:.3f} [ground-truth: {nde01_gt[1][1]:.3f}]\" )\n",
    "print( f\"After change P(fire)={nde01[1][0]:.3f} [ground-truth: {nde01_gt[1][0]:.3f}].\" )\n",
    "print( f\"Thus fire-risc increased in total (directly + drought + temperature + interactions) by {nde01[1][0] - nde01[1][1]:.3f} [gt: {nde01_gt[1][0] - nde01_gt[1][1]:.3f}].\\n\" )\n",
    "\n",
    "total_effect = nde01[1][0] - nde01[1][1]\n",
    "total_effect_gt = nde01_gt[1][0] - nde01_gt[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce19575",
   "metadata": {},
   "source": [
    "#### (e) Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( \"Overview (estimated)\" )\n",
    "print( f\"Direct: {100.0*both_blocked:.1f}%\" )\n",
    "print( f\"Via Drought: {100.0*M1_blocked:.1f}%\" ) # via M2, ie M1 blocked\n",
    "print( f\"Via Temperature: {100.0*M2_blocked:.1f}%\" )\n",
    "print( f\"With Interaction and Mediator->Mediator effects: {100.0*total_effect:.1f}%\" )\n",
    "\n",
    "print(\"\")\n",
    "print( \"Overview (ground-truth)\" )\n",
    "print( f\"Direct: {100.0*both_blocked_gt:.1f}%\" )\n",
    "print( f\"Via Drought: {100.0*M1_blocked_gt:.1f}%\" ) # via M2, ie M1 blocked\n",
    "print( f\"Via Temperature: {100.0*M2_blocked_gt:.1f}%\" )\n",
    "print( f\"With Interaction and Mediator->Mediator effects: {100.0*total_effect_gt:.1f}%\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e82ed5",
   "metadata": {},
   "source": [
    "So overall this would lead us to (correctly) conclude, that the important effects are through temperature and interaction.\n",
    "\n",
    "Note: Estimation near 50% is usually better (as expected, because individual outcomes are less important, as there are already many 0s and many 1s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb51e53",
   "metadata": {},
   "source": [
    "### 3.4 Temperature-Effects\n",
    "What about \"sub\" effects in the middle of the graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d8efa",
   "metadata": {},
   "source": [
    "#### (a) Analyize the situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0627e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator5 = mediation.NaturalEffects_GraphMediation(graph, graph_type, tau_max, fit_setup, world.Observables(),\n",
    "# Note, that we are using a different source this time:\n",
    "                                                  effect_source=M1.Lag(2), effect_target=Y,\n",
    "                                                  blocked_mediators=\"all\", adjustment_set=\"auto\")\n",
    "\n",
    "estimator5.PrintInfo()\n",
    "    \n",
    "print(\"Analyize where data is available:\")\n",
    "x = world.data[M1]\n",
    "counts, bins = np.histogram(x[np.logical_and(x > -3.0, x < 3.0)], 50)\n",
    "max_counts = np.max(counts)\n",
    "plt.stairs(counts/max_counts, bins, color='tab:gray', label=\"Data Availabililty (est. from x only)\")   \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba1bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick some region where estimation seems reasonably supported by the data:\n",
    "x_min = -2.0\n",
    "x_max = 2.0\n",
    "\n",
    "# Let's block the \"boring ones\" (source and target at different times)\n",
    "blocked_mediators=[M1.Lag(1),Y.Lag(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c831b62a",
   "metadata": {},
   "source": [
    "#### (b) Get an estimate of the NDE as function of the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bce1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator5 = mediation.NaturalEffects_GraphMediation(graph, graph_type, tau_max, fit_setup, world.Observables(),\n",
    "                                                  effect_source=M1.Lag(2), effect_target=Y,\n",
    "                                                  blocked_mediators=blocked_mediators, adjustment_set=\"auto\")\n",
    "    \n",
    "# Estimate function from data on [x_min, x_max], see above\n",
    "x_values5, nde_est_smoothed5 = estimator5.NDE_smoothed(x_min, x_max, normalize_by_delta=False, cf_delta=.5,\n",
    "                                                   smoothing_gaussian_sigma_in_steps=5, steps=100)\n",
    "\n",
    "# Also generate the ground-truth to compare to\n",
    "x_values_gt, nde_fct_ground_truth = toy_setup.GroundTruth_NDE_fct_auto(x_min, x_max, estimator5, env, model,\n",
    "                                                   cf_delta=0.5, normalize_by_delta=False)\n",
    "\n",
    "# Set up plots\n",
    "plt_info_est = PlotInfo(color=\"tab:blue\", label=\"NDE (Est)\", colorTE=\"dodgerblue\", colorCF=\"skyblue\",\n",
    "                        labelTE=\"TE (Est)\", labelCF=\"CF (Est)\")\n",
    "plt_info_gt = PlotInfo(color=\"tab:orange\", label=\"NDE (GT)\", colorTE=\"orange\", colorCF=\"navajowhite\",\n",
    "                       labelTE=\"TE (GT)\", labelCF=\"CF (GT)\")\n",
    "\n",
    "plt_data_est = PlotInfo(x=x_values5, y=nde_est_smoothed5, **plt_info_est)\n",
    "plt_data_gt = PlotInfo(x=x_values_gt, y=nde_fct_ground_truth, **plt_info_gt)\n",
    "\n",
    "# Plot the results\n",
    "fig = toy_setup.PlotAbsProbabilities(plt, Y, [plt_data_est, plt_data_gt],\n",
    "                labels=PlotInfo(x=\"x\", y=\"P(Y={cY})\",\n",
    "                title=r\"Values of $P(Y_{x',M_x})$ (light) and total effect $= P(Y_{x,M_x})$ (dark) as functions of $x$\"))\n",
    "\n",
    "fig = toy_setup.PlotChangeInProbabilities(plt, Y, [plt_data_est, plt_data_gt],\n",
    "                labels=PlotInfo(x=\"x\", y=\"ΔP(Y={cY})\",\n",
    "                title=r\"Change in $P(Y_{x', M_x})$ from changing $x'$, as function of $x$\"))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b1c860",
   "metadata": {},
   "source": [
    "#### (c) Results\n",
    "After smoothing, \"sharp\" features of the ground-truth will obviously be lost, but the qualitative behavior is captured ok."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d129dbea",
   "metadata": {},
   "source": [
    "# Example 6: Unidentified Effects <a id='ex6'></a>\n",
    "This example is similar to the one above, but not as a timeseries. This makes the graph simpler, but also removes some knowledge, so that not all effects are identifiable anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb3501",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = toy_setup.CategoricalVariable(\"Clouded\", categories=2)\n",
    "M1 = toy_setup.ContinuousVariable(\"Temperature\")\n",
    "M2 = toy_setup.CategoricalVariable(\"Drought\", categories=2)\n",
    "Y = toy_setup.CategoricalVariable(\"Wild-Fire\", categories=2)\n",
    "\n",
    "env = toy_setup.Environment(exogenous_noise={\n",
    "    X: lambda rng, N: 1.0 * rng.standard_normal(N),\n",
    "    M1: lambda rng, N: 1.0 * rng.standard_normal(N),\n",
    "    M2: lambda rng, N: 1.0 * rng.standard_normal(N),\n",
    "    Y: lambda rng, N: 1.0 * rng.standard_normal(N)\n",
    "}, N=5000, seed=11)\n",
    "model = toy_setup.Model(sem={\n",
    "    X: lambda value: value[X.Noise()] > 1.0,\n",
    "    M1: lambda value: (value[X] + 1.0) * value[M1.Noise()],\n",
    "    M2: lambda value: value[X] + value[M1] + value[M2.Noise()] > 1.0,\n",
    "    Y: lambda value: np.logical_or( value[M1] + value[Y.Noise()] > 3.0,\n",
    "                                   np.logical_and( value[M2], value[M1] + value[Y.Noise()] > 1.5 ) )\n",
    "})\n",
    "\n",
    "graph, graph_type = model.GetGroundtruthGraph()\n",
    "graph = graph.reshape(graph.shape[0:2])\n",
    "\n",
    "print(f\"Graph-Type is '{graph_type}'\")\n",
    "tp.plot_graph(graph = graph,\n",
    "        figsize = (5, 2),\n",
    "        var_names = [\"X\", \"M1\", \"M2\", \"Y\"]\n",
    "        ); plt.show()\n",
    "\n",
    "\n",
    "world = toy_setup.World(env, model)\n",
    "\n",
    "\n",
    "fit_setup = mediation.FitSetup(mediation.FitProvider_Continuous_Default.UseSklearn(20))\n",
    "tau_max = 2\n",
    "estimator = mediation.NaturalEffects_GraphMediation(graph, graph_type, tau_max, fit_setup, world.Observables(),\n",
    "                                                  effect_source=X, effect_target=Y,\n",
    "                                                  blocked_mediators=\"all\", adjustment_set=\"auto\")\n",
    "\n",
    "nde01 = estimator.NDE(False, True)\n",
    "nde01_gt = toy_setup.GroundTruth_NDE(False, True, X, Y, [M1,M2], env, model)\n",
    "print( \"Estimated NDE (blocking both mediators) of changing X=Clouded x=False to x=True is:\" )\n",
    "print( f\"Before change P(fire)={nde01[1][1]:.3f} [gt: {nde01_gt[1][1]:.3f}] and after change P(fire)={nde01[1][0]:.3f}  [gt: {nde01_gt[1][0]:.3f}].\" )\n",
    "print( f\"Thus fire-risc increased as a direct effect of non-cloudiness by {nde01[1][0] - nde01[1][1]:.3f} [gt: {nde01_gt[1][0] - nde01_gt[1][1]:.3f}].\\n\" )\n",
    "\n",
    "\n",
    "estimator2 = mediation.NaturalEffects_GraphMediation(graph, graph_type, tau_max, fit_setup, world.Observables(),\n",
    "                                                  effect_source=X, effect_target=Y,\n",
    "                                                  blocked_mediators=[M1], adjustment_set=\"auto\")\n",
    "\n",
    "nde01 = estimator2.NDE(0, 1) # Writing False/True or 0/1 should both work just fine ...\n",
    "nde01_gt = toy_setup.GroundTruth_NDE(False, True, X, Y, [M1], env, model)\n",
    "print( \"Estimated NDE (blocking only M1=temperature) of changing X=Clouded x=False to x=True is:\" )\n",
    "print( f\"Before change P(fire)={nde01[1][1]:.3f} [gt: {nde01_gt[1][1]:.3f}] and after change P(fire)={nde01[1][0]:.3f}  [gt: {nde01_gt[1][0]:.3f}].\" )\n",
    "print( f\"Thus fire-risc increased as a direct+drought-induced effect of non-cloudiness by {nde01[1][0] - nde01[1][1]:.3f} [gt: {nde01_gt[1][0] - nde01_gt[1][1]:.3f}].\\n\" )\n",
    "\n",
    "try:\n",
    "    print( \"While for blocking only M2=Drought ... (which is a 'kite'-graph / M1 is a 'recanting witness')\" )\n",
    "    estimator3 = mediation.NaturalEffects_GraphMediation(graph, graph_type, tau_max, fit_setup, world.Observables(),\n",
    "                                                      effect_source=X, effect_target=Y,\n",
    "                                                      blocked_mediators=[M2], adjustment_set=\"auto\")\n",
    "except Exception as e:\n",
    "    print(f\"Got exception: {e.args[0]}\")\n",
    "nde01_gt = toy_setup.GroundTruth_NDE(False, True, X, Y, [M2], env, model)\n",
    "print(f\"[Ground-Truth would have been: from {nde01_gt[1][1]:.3f} to {nde01_gt[1][0]:.3f}, difference {nde01_gt[1][0] - nde01_gt[1][1]:.3f}]]\")\n",
    "    \n",
    "estimator4 = mediation.NaturalEffects_GraphMediation(graph, graph_type, tau_max, fit_setup, world.Observables(),\n",
    "                                                  effect_source=X, effect_target=Y,\n",
    "                                                  blocked_mediators=[], adjustment_set=\"auto\",\n",
    "                                                  fall_back_to_total_effect=True)\n",
    "\n",
    "nde01 = estimator4.NDE(False, True)\n",
    "nde01_gt = toy_setup.GroundTruth_NDE(False, True, X, Y, [], env, model)\n",
    "print( \"\\nEstimated total-effect of changing X=Clouded x=False to x=True is:\" )\n",
    "print( f\"Before change P(fire)={nde01[1][1]:.3f} [gt: {nde01_gt[1][1]:.3f}] and after change P(fire)={nde01[1][0]:.3f}  [gt: {nde01_gt[1][0]:.3f}].\" )\n",
    "print( f\"Thus fire-risc increased in total (directly + drought + temperature + interactions) by {nde01[1][0] - nde01[1][1]:.3f} [gt: {nde01_gt[1][0] - nde01_gt[1][1]:.3f}].\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3393185f",
   "metadata": {},
   "source": [
    "# Technical Appendix: <a id='appendix'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfc5280",
   "metadata": {},
   "source": [
    "# Example A: Toymodels, Method Validation and Groundtruth <a id='exA'></a>\n",
    "This example quickly demonstrates how we validate our methods and aims to give an impression of what accuracy and results to expect.\n",
    "For models and effects of interest, see the examples below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e53aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ContinuousVariable(name=\"Treatment\")\n",
    "M = CategoricalVariable(name=\"Mediator\", categories=5)\n",
    "Y = ContinuousVariable(name=\"Target\")\n",
    "env = Environment(exogenous_noise = {\n",
    "    A : lambda rng, N : rng.standard_normal(N),    \n",
    "    M : lambda rng, N : rng.binomial(M.categories-1,0.7,N),\n",
    "    Y : lambda rng, N : rng.standard_normal(N)\n",
    "}, N=10000)\n",
    "model = Model(sem = {\n",
    "    A : lambda value : value[A.Noise()],\n",
    "    M : lambda value : np.clip( np.round( value[M.Noise()] + value[A] ).astype( dtype=np.dtype(\"int32\") ), 0, M.categories-1 ),\n",
    "    Y : lambda value : value[Y.Noise()] + 1.5 * value[A] + value[M] * value[A]\n",
    "})\n",
    "\n",
    "fit_setup = mediation.FitSetup()\n",
    "\n",
    "def ComputeNDE(env):\n",
    "    world = World(env,model)\n",
    "    estimator = mediation.NaturalEffects_StandardMediationSetup(fit_setup, source=A, target=Y, mediator=M,\n",
    "                                            data = world.Observables())\n",
    "    return estimator.NDE(0.0,1.0)\n",
    "\n",
    "model0 = model.Intervene(changes = { A : 0.0 })\n",
    "model1 = model.Intervene(changes = { A : 1.0 })\n",
    "\n",
    "def ComputeGroundTruth01(env):\n",
    "    # This and similar functionality are also available in \"toy_setup.py\"\n",
    "    world0 = toy_setup.World(env,model0)\n",
    "    world1 = toy_setup.World(env,model1)\n",
    "    cf_world = toy_setup.CounterfactualWorld(env,model)\n",
    "    cf_world.TakeVariablesFromWorld(world0,[M])\n",
    "    cf_world.TakeVariablesFromWorld(world1,[A])\n",
    "    cf_world.Compute()    \n",
    "    return np.mean(cf_world.data[Y] - world0.data[Y])\n",
    "\n",
    "runs = 1000\n",
    "ensemble = toy_setup.Ensemble(shared_setup=env, payloads=[ComputeNDE, ComputeGroundTruth01], runs=runs)\n",
    "\n",
    "def PlotTestResults(results,bins=50):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    counts, bins = np.histogram(results[0], bins)\n",
    "    ax1.stairs(counts, bins, color='tab:blue', label=\"Frequency of estimation result\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    counts, bins = np.histogram(results[1], bins)\n",
    "    ax2.stairs(counts, bins, color='tab:red', label=\"Frequency of ground-truth result\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.legend()\n",
    "    plt.title(f\"Ensemble of {runs} runs with {env.N} datapoints each.\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "PlotTestResults(ensemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c117c5",
   "metadata": {},
   "source": [
    "# Example B: Customize Fitting <a id='exB'></a>\n",
    "The current implementation comes with customizable \"fit-setups\" for changing the behavior in \"actual\" (function-level) fits. Below are, in increasing complexity, extension-points that the current implementation provides."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7473a82e",
   "metadata": {},
   "source": [
    "## B.1 The default setup\n",
    "This is the easiest to use, it uses Sklearn's KNeighborsRegressor with 10 neighbors and uniform weigths for mappings and Sklearn's KernelDensity with gaussian kernel and bandwidth 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bc0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_setup = mediation.FitSetup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aead1cf",
   "metadata": {},
   "source": [
    "## B.2 Default setup with custom parameters\n",
    "Use the defaults, but with other parameters (or with GaussianProcessRegressor instead of KNeighborsRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4fcb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_setup = mediation.FitSetup(\n",
    "    fit_map = mediation.FitProvider_Continuous_Default.UseSklearn(neighbors=10, weights='uniform'),\n",
    "    fit_density = mediation.FitProvider_Density_Default.UseSklearn(kernel='gaussian', bandwidth=0.2)\n",
    ")\n",
    "\n",
    "# or using KNeighborsRegressor (arguments passed to UseSklearn_GC(...) are forwarded to sklearn)\n",
    "fit_setup = mediation.FitSetup(\n",
    "    fit_map = mediation.FitProvider_Continuous_Default.UseSklearn_GC(),\n",
    "    fit_density = mediation.FitProvider_Density_Default.UseSklearn(kernel='gaussian', bandwidth=0.2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcd79ca",
   "metadata": {},
   "source": [
    "## B.3 Default setup with custom fits\n",
    "Use the default setup, but provide any callable that given data (see B.4 below) returns an function-object f, that can be called via f(x) or f.predict(x) (see B.4 below).\n",
    "Maps can be picked separately for one-dimensional fits (see B.4 below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f20a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_map = mediation.FitProvider_Continuous_Default(\n",
    "    # e.g.:\n",
    "    lambda x, y: KNeighborsRegressor(n_neighbors=10, weights='uniform').fit(x, y)\n",
    ")\n",
    "fit_map2 = mediation.FitProvider_Continuous_Default(\n",
    "    # e.g.:\n",
    "    lambda x, y: KNeighborsRegressor(n_neighbors=30, weights='uniform').fit(x, y),\n",
    "    # 1d fits are different, could e.g. use splines instead\n",
    "    fit_map_1d = \n",
    "    lambda x, y: KNeighborsRegressor(n_neighbors=5, weights=weights).fit(x, y)\n",
    ")\n",
    "\n",
    "fit_density = mediation.FitProvider_Density_Default(\n",
    "    # e.g. (but could also use normalizing flows etc)\n",
    "    lambda x: KernelDensity(kernel='gaussian', bandwidth=0.2).fit(x)\n",
    ")\n",
    "\n",
    "# or using KNeighborsRegressor (arguments passed to UseSklearn_GC(...) are forwarded to sklearn)\n",
    "fit_setup = mediation.FitSetup(\n",
    "    fit_map = fit_map,\n",
    "    fit_density = fit_density\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42dde17",
   "metadata": {},
   "source": [
    "## B.4 Default Setup with Custom fit provider\n",
    "The \"fit providers\" set up above can also be overwritten, the current implementation uses. A custom implementation has to provide the interface containing 'def Get_Fit_Continuous(x, y):', or 'Get_Fit_Density(x_train)', see below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5837cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitProvider_Continuous_Default_Tutorial:\n",
    "    def __init__(self, fit_map, fit_map_1d=None):\n",
    "        self.fit_map = fit_map\n",
    "        self.fit_map_1d = fit_map_1d\n",
    "\n",
    "    @classmethod\n",
    "    def UseSklearn(cls, neighbors=10):\n",
    "        return cls(lambda x, y: KNeighborsRegressor(n_neighbors=neighbors, weights='uniform').fit(x, y))\n",
    "\n",
    "    @classmethod\n",
    "    def UseSklearn_GC(cls, neighbors=10):\n",
    "        from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "        return cls(lambda x, y: GaussianProcessRegressor().fit(x, y))\n",
    "\n",
    "    def Get_Fit_Continuous(self, x, y):\n",
    "        dim_of_domain = np.shape(x)[1]\n",
    "        if dim_of_domain == 0:\n",
    "            return lambda x_dim_zero: np.mean(y)\n",
    "        elif dim_of_domain == 1 and self.fit_map_1d is not None:\n",
    "            return self.fit_map_1d(x, y)  # use splines?\n",
    "        else:\n",
    "            return self.fit_map(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef925a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitProvider_Density_Default_Tutorial:\n",
    "    def __init__(self, fit_density):\n",
    "        self.fit_density = fit_density\n",
    "\n",
    "    @classmethod\n",
    "    def UseSklearn(cls, kernel='gaussian', bandwidth=0.2):\n",
    "        return cls(lambda x: KernelDensity(kernel=kernel, bandwidth=bandwidth).fit(x))\n",
    "\n",
    "    def Get_Fit_Density(self, x_train):\n",
    "        dim_of_domain = np.shape(x_train)[1]\n",
    "        if dim_of_domain == 0:\n",
    "            return lambda x_dim_zero: np.ones(np.shape(x_dim_zero)[0])\n",
    "        elif np.shape(x_train)[0] > 0:\n",
    "            model = self.fit_density(x_train)\n",
    "            return lambda x_predict: np.exp(model.score_samples(x_predict))\n",
    "        else:\n",
    "            return lambda x_predict: np.zeros(np.shape(x_predict)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0614a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_setup = mediation.FitSetup(\n",
    "    fit_map = FitProvider_Continuous_Default_Tutorial.UseSklearn(),\n",
    "    fit_density = FitProvider_Density_Default_Tutorial.UseSklearn()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60872fab",
   "metadata": {},
   "source": [
    "## B.5 Custom fit-setup\n",
    "The current fit-setup implementation is based on transfer-matrices, and uses the two fit providers for maps and densities as outlined above, to extend this approach from categorical data to mixed data.\n",
    "\n",
    "There are of course alternatives to this approach, for example one-hot-encoding, that come with their own advantages and disadvantages. Any such setup can be used with the mediation-analysis implementations above, it has to expose a .Fit as outline below (see also mediation.py -> FitSetup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afac4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitSetup_Tutorial:\n",
    "    def Fit(self, domain, target, dont_wrap_1d_y_in_vector=True):\n",
    "        # domain is a dictionary { var-ids-domain : data } of the domain-variables\n",
    "        # target is a dictionary { var-ids-target : data } of the target-variables\n",
    "        # (var ids are toy_setup.CategoricalVariable or toy_setup.ContinuousVariable, check e.g. via var.is_categorical,\n",
    "        #  and contain additional information about variables, e.g. var.categories or var.dimension, cf toy_setup.py)\n",
    "        # dont_wrap_1d_y_in_vector indicates wheter to return scalar targets as scalars or 1d-vectors\n",
    "        \n",
    "        # return a function object f that can be called via\n",
    "        # f({ var-ids-domain : data }) or via\n",
    "        # f.predict({ var-ids-domain : data })\n",
    "        # and returning [a python iterable with one entry per target Y (see dont_wrap_1d_y_in_vector)]\n",
    "        # with entries np.arrays of shape:\n",
    "        # [data-points in 'data', dimension of Y] if Y is continuous, containing expectations-values\n",
    "        # [data-points in 'data', categories of Y] if Y is categorical, containing category-probabilities\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32782043",
   "metadata": {},
   "source": [
    "# Example C: Inspecting Fits <a id='exC'></a>\n",
    "From the code in nde.py, it is usually easy to see which fits actually get executed, these can be inspected e.g. via:\n",
    "(see also Example 0 for possible issues, that might require data-normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a8a0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = toy_setup.CategoricalVariable(\"Clouded\", categories=2)\n",
    "M1 = toy_setup.ContinuousVariable(\"Temperature\")\n",
    "M2 = toy_setup.CategoricalVariable(\"Drought\", categories=2)\n",
    "Y = toy_setup.CategoricalVariable(\"Wild-Fire\", categories=2)\n",
    "\n",
    "env = toy_setup.Environment(exogenous_noise={\n",
    "    X: lambda rng, N: 1.0 * rng.standard_normal(N),\n",
    "    M1: lambda rng, N: 1.0 * rng.standard_normal(N),\n",
    "    M2: lambda rng, N: 1.0 * rng.standard_normal(N),\n",
    "    Y: lambda rng, N: 1.0 * rng.standard_normal(N)\n",
    "}, N=5000, seed=11)\n",
    "autocorr = 0.3\n",
    "model = toy_setup.Model(sem={\n",
    "    X: lambda value: autocorr * value[X.Lag(1)] + value[X.Noise()] > 1.0,\n",
    "    M1: lambda value: autocorr * value[M1.Lag(1)] + (2.0 * value[X.Lag(1)] - 1.0) + value[M1.Noise()],\n",
    "    M2: lambda value: autocorr * value[M2.Lag(1)] + value[X.Lag(1)] + value[M1.Lag(1)] + value[M2.Noise()] > 1.5,\n",
    "    Y: lambda value: np.logical_or(autocorr * value[Y.Lag(1)] + value[M1.Lag(1)] + value[Y.Noise()] > 2.5,\n",
    "                                   np.logical_and(value[M2.Lag(1)],\n",
    "                                                  autocorr * value[Y.Lag(1)] + 5.0 * value[M1.Lag(1)] + value[\n",
    "                                                      Y.Noise()] > 2.5))\n",
    "})\n",
    "\n",
    "\n",
    "fit = mediation.FitSetup(mediation.FitProvider_Continuous_Default.UseSklearn(20))\n",
    "\n",
    "# World for training data\n",
    "world_train = toy_setup.World(env, model)\n",
    "\n",
    "M1_past = toy_setup.ContinuousVariable(\"T_past\")\n",
    "M2_past = toy_setup.CategoricalVariable(\"D_past\", categories=2)\n",
    "Y_past = toy_setup.CategoricalVariable(\"F_past\", categories=2)\n",
    "obs = world_train.Observables()\n",
    "\n",
    "# Do Fit\n",
    "f = fit.Fit_CategoricalTarget_MarkovKernel({M1_past: obs[M1][0:-1],\n",
    "                                             M2_past:   obs[M2][0:-1],\n",
    "                                             Y_past:   obs[Y][0:-1]},\n",
    "                                           obs[Y][1:], Y.categories)\n",
    "\n",
    "# World for test data\n",
    "env.ResetWithNewSeed(77)\n",
    "world_test = toy_setup.World(env, model)\n",
    "obs = world_test.Observables()\n",
    "\n",
    "\n",
    "# Predict on test data\n",
    "predictions = f({M1_past: obs[M1][0:-1], M2_past: obs[M2][0:-1], Y_past: obs[Y][0:-1]})\n",
    "predictions_mean = np.mean(predictions, axis=0)\n",
    "\n",
    "print(\"Predictions: \" + str(predictions_mean))\n",
    "\n",
    "# Compare to actual values\n",
    "actual = np.zeros(Y.categories)\n",
    "for cY in range(Y.categories):\n",
    "    actual[cY] = np.count_nonzero(obs[Y] == cY) / env.N\n",
    "print(\"Actual: \" + str(actual))\n",
    "err_abs_max = np.max(np.abs(predictions_mean - actual))\n",
    "assert err_abs_max < 0.01, \"Prediction did not agree with ground-truth \" + str(np.abs(predictions_mean - actual))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492dfd3e",
   "metadata": {},
   "source": [
    "For example the default choice, using nearest neighbors, might perform poorly on data with \"sharp\" features, this can often be seen easily (one could of course also plot vs observations, rather than vs ground-truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6086b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_setup = mediation.FitSetup()\n",
    "X1 = toy_setup.CategoricalVariable(categories=2)\n",
    "X2 = toy_setup.ContinuousVariable()\n",
    "Y = toy_setup.CategoricalVariable(categories=3)\n",
    "env = toy_setup.Environment(exogenous_noise={\n",
    "    X1: lambda rng, N: rng.binomial(X1.categories - 1, 0.4, N),\n",
    "    X2: lambda rng, N: rng.standard_normal(N),\n",
    "    Y: lambda rng, N: rng.binomial(Y.categories - 1, 0.7, N),\n",
    "}, N=1000, seed=55)\n",
    "model = toy_setup.Model(sem={\n",
    "    X1: lambda value: value[X1.Noise()],\n",
    "    X2: lambda value: value[X2.Noise()],\n",
    "    Y: lambda value: np.clip(value[Y.Noise()] + 1.75 * value[X1] + value[X2] * value[X2],\n",
    "                             0, Y.categories-1).astype(dtype=\"int32\")\n",
    "})\n",
    "\n",
    "world_train = toy_setup.World(env, model)\n",
    "f = fit_setup.Fit({X1: world_train.data[X1], X2: world_train.data[X2]}, {Y: world_train.data[Y]})\n",
    "\n",
    "env.ResetWithNewSeed(77)\n",
    "world_test = toy_setup.World(env, model)\n",
    "predictions = np.mean(f({X1: world_test.data[X1], X2: world_train.data[X2]}), axis=0)\n",
    "print(\"Predictions: \" + str(predictions))\n",
    "actual = np.zeros(Y.categories)\n",
    "for cY in range(Y.categories):\n",
    "    actual[cY] = np.count_nonzero(world_test.data[Y] == cY) / env.N\n",
    "print(\"Actual: \" + str(actual))\n",
    "err_abs_max = np.max(np.abs(predictions - actual))\n",
    "assert err_abs_max < 0.01, \"Prediction did not agree with ground-truth \" + str(np.abs(predictions - actual))\n",
    "\n",
    "# Note that noise on Y is binomial, so only discrete steps happen in the ground-truth\n",
    "for cX in range(X1.categories):\n",
    "    x2 = np.arange(-2.0, 2.0, 0.1)\n",
    "    y = f({X1: np.full_like(x2, cX, dtype=np.dtype(\"uint32\")), X2: x2})\n",
    "    plt.plot(x2,y)\n",
    "    noise = env.GetNoise()\n",
    "    y_noise = noise[Y.Noise()]\n",
    "    result = []\n",
    "    for x2value in x2:\n",
    "        y_values = model.SEM[Y](\n",
    "            {X1: np.full_like(y_noise, cX, dtype=np.dtype(\"uint32\")),\n",
    "             X2: np.full_like(y_noise, x2value, dtype=np.dtype(\"float32\")),\n",
    "             Y.Noise(): y_noise\n",
    "        })\n",
    "        this_point_p = []\n",
    "        for cY in range(Y.categories):\n",
    "            this_point_p.append(np.count_nonzero(y_values == cY))\n",
    "        result.append(this_point_p)\n",
    "    plt.plot(x2, np.array(result)/env.N)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39new",
   "language": "python",
   "name": "py39new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
